{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"ab0003007f134b199bea3f68aee5f0af","deepnote_cell_type":"markdown"},"source":"<h1>ITNPDB2 Representing and Manipulating Data</h1>\n<h3>University of Stirling<br>Dr. Saemundur Haraldsson</h3>","block_group":"ab0003007f134b199bea3f68aee5f0af"},{"cell_type":"markdown","metadata":{"cell_id":"b6631b0273cc47b5821b4bef1dd41de6","deepnote_cell_type":"markdown"},"source":"<h1>SciKit-Learn: Overview</h1>\n<h2>Machine Learning <a href=\"https://scikit-learn.org/stable/\">(documentation)</a></h2>\n<h4>        \n    <ul>\n        <li>Preprocessing</li>\n        <li>Model selection</li>\n        <li>Classification</li>\n        <li>Regression</li>\n        <li>Clustering</li>\n        <li>Dimensionality reduction</li>\n        </ul>\n</h4>","block_group":"b6631b0273cc47b5821b4bef1dd41de6"},{"cell_type":"code","metadata":{"cell_id":"4a2634b7d5ec48ab9780b0f83dada8e5","deepnote_cell_type":"code"},"source":"# First we need to import some base modules we'll need in this lecture\nimport numpy\n# We'll need to generate some data to play with\nfrom scipy import stats\n# We'll also need predefined data from SciKit-Learn\nfrom sklearn import datasets\n# Just a stuff we need to make pretty data and things\nfrom operator import itemgetter\n\nimport matplotlib.pyplot as plt\nnumpy.random.seed(seed=1234) # So that the lecture becomes deterministic","block_group":"4a2634b7d5ec48ab9780b0f83dada8e5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"fe72f214a711405fab743fdc0737d9ff","deepnote_cell_type":"markdown"},"source":"<h3>Preprocessing</h3>\n<h4>Most real world data cannot be use as is.<br>\n    We'll need to \"clean\" the data by\n<ul>\n    <li>Standardise or normalise</li>\n    <li>Deal with outliers</li>\n    <li>Encode categorical features</li>\n</ul>\n</h4>\n<h3>SciKit-Learn provides utility functions and classes to do this for us</h3>","block_group":"fe72f214a711405fab743fdc0737d9ff"},{"cell_type":"code","metadata":{"cell_id":"093bccd51a63414897fe5bc6f1a92703","deepnote_cell_type":"code"},"source":"from sklearn import preprocessing\n# First make some data\nX_train = numpy.array([stats.alpha.rvs(7,size=10),\n                       stats.anglit.rvs(size=10),\n                       stats.geom.rvs(.8,size=10)\n                      ]).transpose()\nX_test = numpy.array([stats.alpha.rvs(7,size=5),\n                       stats.anglit.rvs(size=5),\n                       stats.geom.rvs(.8,size=5)\n                      ]).transpose()\n\nprint(X_train)\n\n#Now we can scale it to a mean 0 and a unit variance\n#X_scaled = preprocessing.scale(X_train)\n\nX_scaled = preprocessing.scale(X_train)\nprint(X_scaled)","block_group":"093bccd51a63414897fe5bc6f1a92703","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"a3d24bcfb48447b8aba293c68e238011","deepnote_cell_type":"markdown"},"source":"<h3> See the difference</h3>","block_group":"a3d24bcfb48447b8aba293c68e238011"},{"cell_type":"code","metadata":{"cell_id":"701095deeb8542d5849e0d4f6f1dca49","deepnote_cell_type":"code"},"source":"print('mean changes from ',X_train.mean(axis=0), ' --> ', X_scaled.mean(axis=0))\nprint('Variance changes from ',X_train.std(axis=0), ' --> ',X_scaled.std(axis=0))","block_group":"701095deeb8542d5849e0d4f6f1dca49","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"bcd64aa474894ea8a30def2e5c6c31d6","deepnote_cell_type":"markdown"},"source":"<h3> We should keep track of our scalers so that we can treat the training set equally</h3>\n<h4> For that we can use the class StandardScaler<br>\nThere are others as well:\n    <ul>\n        <li>Normalizer -- Scaling to unit norm</li>\n        <li>MinMaxScaler -- Squeese the data between min-max values</li>\n        <li>MaxAbsScaler -- Squeese the data below a certain max value (defaults to 1)</li>\n        <li>RobustScaler -- For when you have many outliers</li>\n        <li>QuantileTransformer -- Transforming to a uniform distribution</li>\n        <li>PowerTransformer -- Map from \"any\" distribution to Gaussian</li>\n    </ul>\n\n</h4>","block_group":"bcd64aa474894ea8a30def2e5c6c31d6"},{"cell_type":"code","metadata":{"cell_id":"3d083042a28e471c8689e26f286c4785","deepnote_cell_type":"code"},"source":"scaler = preprocessing.StandardScaler().fit(X_train)\n# The scaler keeps track of the original mean and variance\nprint('mean: ', scaler.mean_,' and scale: ',scaler.scale_)\n# and we can use the scaler instance to scale the test data in the same way\nprint(X_test)\nX_test_scaled = scaler.transform(X_test)\nprint(X_test_scaled)\nprint('mean changes from ',X_test.mean(axis=0), ' --> ', X_test_scaled.mean(axis=0))\nprint('Variance changes from ',X_test.std(axis=0), ' --> ',X_test_scaled.std(axis=0))","block_group":"3d083042a28e471c8689e26f286c4785","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"1397db6947274860a5cbdb54a574df27","deepnote_cell_type":"markdown"},"source":"<h3>Now let's try squeesing the data between a given min-max value</h3>\n<h4>We know that the mean changes but what do you think happens to the variance?</h4>","block_group":"1397db6947274860a5cbdb54a574df27"},{"cell_type":"code","metadata":{"cell_id":"0ca187e4dfd344d4be093d4b952640b8","deepnote_cell_type":"code"},"source":"# First just the standard\nmm_scaler = preprocessing.MinMaxScaler()\nX_scaled = mm_scaler.fit_transform(X_train)\nprint(X_scaled)\nprint('The variance was: ',X_train.std(axis=0))\nprint('      but is now: ',X_scaled.std(axis=0))\n\n# Now we force it between 1 and 3\nmm_scaler = preprocessing.MinMaxScaler(feature_range=(1,3))\nX_scaled = mm_scaler.fit_transform(X_train)\nprint(X_scaled)\nprint('The variance was: ',X_train.std(axis=0))\nprint('      but is now: ',X_scaled.std(axis=0))","block_group":"0ca187e4dfd344d4be093d4b952640b8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"d543372f6e5743ddb7dedd491bb23dc9","deepnote_cell_type":"markdown"},"source":"<h3>What if we have categorical data</h3>\n<h4>i.e. post codes, education, gender identidy, etc.</h4>\n<ul>\n    <li><h4>OrdinalEncoder --- <i>Changes the categories to integers</i></h4></li>\n    <li><h4>OneHotEncoder --- <i>Creates a binary (dummy) feature for each category</i></h4></li>\n</ul>\n<p>Note that Pandas provide a more convenient version of this functionality in Dataframes</p>","block_group":"d543372f6e5743ddb7dedd491bb23dc9"},{"cell_type":"code","metadata":{"cell_id":"65e37f8b3e754d7eae3210bf0e0ba4fd","deepnote_cell_type":"code"},"source":"# The data\nX_train = [['male', 'from US', 'uses Safari'], \n               ['female', 'from Europe', 'uses Firefox'],\n               ['female', 'from Africa', 'uses Opera'],\n               ['male', 'from Europe', 'uses Opera']\n              ]\nX_test = [['female', 'from Africa', 'uses Safari'], \n          ['male', 'from US', 'uses Opera']\n         ]","block_group":"65e37f8b3e754d7eae3210bf0e0ba4fd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"8bf7c1a4e119417398d6631d0fcc4b84","deepnote_cell_type":"markdown"},"source":"## OrdinalEncoder","block_group":"8bf7c1a4e119417398d6631d0fcc4b84"},{"cell_type":"code","metadata":{"cell_id":"c1362dd79df24f68b0de57bed5a60215","deepnote_cell_type":"code"},"source":"enc_ord = preprocessing.OrdinalEncoder()\nX_ordinal = enc_ord.fit(X_train).transform(X_train)\nprint(X_ordinal)\nprint(enc_ord.transform(X_test))","block_group":"c1362dd79df24f68b0de57bed5a60215","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"644e0bb3f83c40a28578a2c794118b38","deepnote_cell_type":"markdown"},"source":"## Create more features","block_group":"644e0bb3f83c40a28578a2c794118b38"},{"cell_type":"code","metadata":{"cell_id":"35fc1a4975cc42c5bcd07bdb2e19f311","deepnote_cell_type":"code"},"source":"enc_dum = preprocessing.OneHotEncoder()\nX_dummies = enc_dum.fit(X_train).transform(X_train).toarray()\nprint(X_dummies)\nprint(enc_dum.transform(X_test).toarray())","block_group":"35fc1a4975cc42c5bcd07bdb2e19f311","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"92e7a48af42d4cf689a103d5290f424b","deepnote_cell_type":"markdown"},"source":"## What if there are more categories in the test set?","block_group":"92e7a48af42d4cf689a103d5290f424b"},{"cell_type":"code","metadata":{"cell_id":"400d712d89c04f418daf0917ac1157f6","deepnote_cell_type":"code"},"source":"X_test_alt = [['female', 'from Asia', 'uses Safari'], \n              ['male', 'from US', 'uses Chrome']\n             ]\n\n# This will fail, toggle comment on second try :) \n#enc_ord.transform(X_test_alt)\n#enc_dum.transform(X_test_alt)\n\nenc_dum = preprocessing.OneHotEncoder(handle_unknown='ignore')\nenc_dum.fit(X_train)\nprint(enc_dum.transform(X_test_alt).toarray())","block_group":"400d712d89c04f418daf0917ac1157f6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"a098aced903f4aa7a177dd823fce4a84","deepnote_cell_type":"markdown"},"source":"<h3>Classification and regression</h3>\n<h4>Different sides of the same coin<br>\nWhat type is the variable you are trying to predict? (i.e. the \"Y\" value)\n</h4>\n<ul>\n    <li><h4>Categorical --- <i>Classification</i></h4></li>\n    <li><h4>Numerical --- </h4></li>\n    <ul>\n        <li>Discreet and finite -- <i>Classification</i> (only a few finite values possible)</li>\n        <li>Discreet and infinite -- <i>Regression</i> (infinite or very large set of values possible)</li>\n        <li>Continuous -- <i>Regression</i> (infinite or very large set of values possible)</li>\n    </ul>\n</ul>\n<h4>Most implemented models in SciKit learn can be used for both Classification and Regression </h4>\n<p>fun fact: The link for classification and the link for regression on scikit-learn's website return the same page</p>\n\n<h3>Clustering</h3>\n<h4>Can be considered a more general way of classification<br>\nyou are automatically trying to learn the categories or labels.\n</h4>\n<h4>Unsupervised learning</h4>","block_group":"a098aced903f4aa7a177dd823fce4a84"},{"cell_type":"markdown","metadata":{"cell_id":"3f24923ab9a948e691eb1dc6be1a0a92","deepnote_cell_type":"markdown"},"source":"<h3>Linear models</h3>\n<h4>Mostly for regression and when the target is expected to be a linear combination of the features</h4>","block_group":"3f24923ab9a948e691eb1dc6be1a0a92"},{"cell_type":"code","metadata":{"cell_id":"2d1cd66920e44894aba93e3c2cdb8ba9","deepnote_cell_type":"code"},"source":"from sklearn import linear_model\n# Let's make some data\nX_train = stats.norm.rvs(size=50).reshape(-1,1)\nY_train = stats.norminvgauss.rvs(1, 0.5,size=50)\nX_test = stats.norm.rvs(size=10).reshape(-1,1)\n\n# Fit to a model -- try different linear models\n#regr = linear_model.LinearRegression()\nregr = linear_model.Ridge(alpha=.5)\nregr.fit(X_train, Y_train)\n\n\n# Plot the model\nY_pred = regr.predict(X_test)\n\nfig = plt.figure(figsize=(12,9))\nax = fig.add_subplot(111)\nax.scatter(X_train,Y_train)\nax.plot(X_test,Y_pred,'r',label=u'Linear model')\nplt.legend()\nplt.show()","block_group":"2d1cd66920e44894aba93e3c2cdb8ba9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"decca25c75f948898a105f3d24802afb","deepnote_cell_type":"markdown"},"source":"<h3>Support Vector Machines</h3>\n<ul>\n    <li><h4>For regression and classification (also outliers detection)</h4></li>\n    <li><h4>Effective on high dimensional data</h4></li>\n    <li><h4>No direct probability estimates</h4></li>\n</ul>","block_group":"decca25c75f948898a105f3d24802afb"},{"cell_type":"code","metadata":{"cell_id":"f434d2e3ad084e8ba5bcff12da1ade97","deepnote_cell_type":"code"},"source":"from sklearn import svm\n# For classification we need a categorical target\nX_train = numpy.array([stats.norm.rvs(size=50),\n                       stats.norminvgauss.rvs(1, 0.5,size=50)\n                      ]).transpose()\nX_train = numpy.array(sorted(X_train,key=itemgetter(0,1)))\nY_train = numpy.array([1]*int(len(X_train)/2)+[0]*int(len(X_train)/2))\nprint(Y_train)\nclf = svm.SVC(gamma='scale')\nclf.fit(X_train,Y_train)\nfig = plt.figure(figsize=(12,9))\nax = fig.add_subplot(111)\nax.plot(X_train[Y_train==1,0],X_train[Y_train==1,1],'r*',markersize=12)\nax.plot(X_train[Y_train==0,0],X_train[Y_train==0,1],'go',markersize=12)\nplt.show()","block_group":"f434d2e3ad084e8ba5bcff12da1ade97","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"c6f6383009a348fd98744290a3ae5f31","deepnote_cell_type":"markdown"},"source":"<ul>\n    <li><h4>When we've fitted the model we can use it to predict</h4></li>\n</ul>","block_group":"c6f6383009a348fd98744290a3ae5f31"},{"cell_type":"code","metadata":{"cell_id":"d39084a8513545f786d4acc6ec79c5cc","deepnote_cell_type":"code"},"source":"X_test =  numpy.array([stats.norm.rvs(size=10),\n                       stats.norminvgauss.rvs(1, 0.5,size=10)\n                      ]).transpose()\nY_pred = clf.predict(X_test)\nfig = plt.figure(figsize=(12,9))\nax = fig.add_subplot(111)\nax.plot(X_train[Y_train==1,0],X_train[Y_train==1,1],'ro',markersize=6,alpha=.3)\nax.plot(X_train[Y_train==0,0],X_train[Y_train==0,1],'go',markersize=6,alpha=.3)\nax.plot(X_test[Y_pred==1,0],X_test[Y_pred==1,1],'r*',markersize=13)\nax.plot(X_test[Y_pred==0,0],X_test[Y_pred==0,1],'g*',markersize=13)\nplt.show()","block_group":"d39084a8513545f786d4acc6ec79c5cc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"8b444e512f944673a8a0cf88c9c6d29e","deepnote_cell_type":"markdown"},"source":"<h3>Model selection</h3>\n<h4>Not all models are equal and neither is data<br>\nWe compare models, validate them, and tune them to the data (choose parameter values)<br>\n    For it we use:\n</h4>\n<ul>\n    <li><h4>Evaluate the performance of models</h4> So that we can compare models</li>\n    <li><h4>Parameter tuning</h4> So that the model/method/algorithm is the best version we can get</li>\n    <li><h4>Cross-validation</h4> To avoid overfitting</li>\n</ul>","block_group":"8b444e512f944673a8a0cf88c9c6d29e"},{"cell_type":"markdown","metadata":{"cell_id":"a041ca91a41745f7bc0e27fec32a0164","deepnote_cell_type":"markdown"},"source":"### Evaluating the performance of a model is easy with Scikit-learn\n- model classes provide the __score__ function for this\n- We can also use all kinds of metrics provided\n - Mean Squared Error\n - F-measure\n - Area under ROC curve\n - etc.\n- There's even a function that prints out a pretty table with a number of metrics\n- All you need is a test sample of the data\n - For obvious reasons you don't want to test on the data you trained on\n - Does anyone know these reasons?\n- Let's test this with the digits dataset and use the train_test_split utility function","block_group":"a041ca91a41745f7bc0e27fec32a0164"},{"cell_type":"code","metadata":{"cell_id":"0699c6ad23b3470399d85c249332c0b1","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn import datasets\n\n# Loading the Digits dataset and flattening the data to be able to use it\ndigits = datasets.load_digits()\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target","block_group":"0699c6ad23b3470399d85c249332c0b1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"2f30487fa7c943019433b61b4a310000","deepnote_cell_type":"markdown"},"source":"- One simple function to split it -- let's go for 60-40 trainint-testin split","block_group":"2f30487fa7c943019433b61b4a310000"},{"cell_type":"code","metadata":{"cell_id":"ba8374f149534fa898447960055db33d","deepnote_cell_type":"code"},"source":"# Split the dataset in two equal parts\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)","block_group":"ba8374f149534fa898447960055db33d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"13cd54f0614e437baac3d923bf956c9c","deepnote_cell_type":"markdown"},"source":"- Fit the model to the training sample","block_group":"13cd54f0614e437baac3d923bf956c9c"},{"cell_type":"code","metadata":{"cell_id":"d48c2613dc084f04bf7c96ab549183a7","deepnote_cell_type":"code"},"source":"# Of course we need to fit a model\nclf = RandomForestClassifier(n_estimators=20)\nclf.fit(X_train, y_train)","block_group":"d48c2613dc084f04bf7c96ab549183a7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"2846ebfb465a477a8419d8a43e69a481","deepnote_cell_type":"markdown"},"source":"- and then evaluate on the testing sample","block_group":"2846ebfb465a477a8419d8a43e69a481"},{"cell_type":"code","metadata":{"cell_id":"907394872390402189e38d7336826d62","deepnote_cell_type":"code"},"source":"print(clf.score(X_test, y_test))\ny_pred = clf.predict(X_test)\nprint(f1_score(y_test,y_pred,average='macro'))\nprint(classification_report(y_test, y_pred))","block_group":"907394872390402189e38d7336826d62","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"caaf6992c5714e47b0a25d3e3651d39c","deepnote_cell_type":"markdown"},"source":"### Parameter tuning\n- Now that we know how to evaluate the performance of our models we might want to adjust them\n - find the ''optimal'' parameters \n- We do that by repeatedly fitting our models with different sets of parameters until we are satisfied with the performance\n - We could to that with an __if__ or a __while__ loop \n - or we could use Scikit-learn's utility which we have to tell\n   - what parameters we want to adjust\n   - what the boundaries or possible values the parameters have","block_group":"caaf6992c5714e47b0a25d3e3651d39c"},{"cell_type":"code","metadata":{"cell_id":"4beeb7c2903c4f718d256f8d0a133e90","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": stats.randint(1, 11),\n              \"min_samples_split\": stats.randint(2, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5, iid=False)\nrandom_search.fit(X, y)\n\nprint(random_search.best_params_)\ny_pred = random_search.predict(X_test)\nprint(classification_report(y_test, y_pred))","block_group":"4beeb7c2903c4f718d256f8d0a133e90","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"5db02236c6374673963c1c0871713cd2","deepnote_cell_type":"markdown"},"source":"### Cross-validation\n- What we actually did in the last cell was Cross-validation\n- We split the data into training and testing\n - So that we can test the fitted model on an unseen sample of the data\n - We could do it with functions from random sampling but Scikit-learn has utility functions to do this for us\n- It's a preferable practice to split the data 3-ways\n - Anyone know why?\n","block_group":"5db02236c6374673963c1c0871713cd2"},{"cell_type":"code","metadata":{"cell_id":"2f6259433f4642fd9d353beeb4b6210d","deepnote_cell_type":"code"},"source":"","block_group":"2f6259433f4642fd9d353beeb4b6210d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"299b89da4bb54648b43197276dbaa79e","deepnote_cell_type":"markdown"},"source":"<h3>Dimensionality reduction</h3>\n<h4>Find the features that are least helpful in explaining the variance of the target variable and remove them<br>\n    Why do we need to remove features or dimensions?</h4>\n<ul>\n    <li><h4>The curse of dimensionality <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">(Wiki)</a></h4></li>\n    <li><h4>Increase efficiency and speed</h4></li>\n</ul>\n<h3>We won't go into details about how it works</h3>","block_group":"299b89da4bb54648b43197276dbaa79e"},{"cell_type":"code","metadata":{"cell_id":"2ee2455ef3ee4854a847309ddf6a0f7d","deepnote_cell_type":"code"},"source":"from sklearn.decomposition import PCA\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\nprint(X.shape)\npca = PCA(n_components=6) # Change this to see what happens to the dimensions\nX_pca = pca.fit_transform(X)\nprint(pca.get_covariance())\nprint(X_pca.shape)","block_group":"2ee2455ef3ee4854a847309ddf6a0f7d","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=41af8bd7-a5ed-4334-a2fe-992dcc7ea742' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.4","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"a9ce0dda156543d69b69a599859243f4","deepnote_execution_queue":[]}}